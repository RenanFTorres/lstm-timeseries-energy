{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQy75p6HhZim"
      },
      "source": [
        "## **Importação das bibliotecas e dados (AEP e Duas Unas)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install ipykernel --break-system-packages\n",
        "# %pip install scikit-learn --break-system-packages\n",
        "# %pip install matplotlib --break-system-packages\n",
        "# %pip install keras --break-system-packages\n",
        "# %pip install tensorflow --break-system-packages\n",
        "# %pip install pandas --break-system-packages\n",
        "# %pip install plotly --break-system-packages\n",
        "# %pip install seaborn --break-system-packages\n",
        "# %pip install nbformat --break-system-packages\n",
        "# %pip install keras --break-system-packages\n",
        "# %pip install pyswarm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.initializers import GlorotUniform    #Inicializador de pesos\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf #Biblioteca para machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D-eSfJgl8qO_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43\n",
            "[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_path = './dataset/'\n",
        "\n",
        "# carregando arquivo CSV\n",
        "time_base = 'days'  # Escolha do tipo de agrupamento\n",
        "duas_unas_dataset = f'{base_path}duas_unas/{time_base}/grouped_3_{time_base}.csv'  # Definição do caminho do diretório do dataset\n",
        "print(len(duas_unas_dataset))\n",
        "validation_interval = 32  # Definição da parcela dos dados que serão retiradas do treinamento/teste para validação\n",
        "time_step = 1  # Number of previous time steps to consider\n",
        "output_steps = 20  # Number of future steps to predict\n",
        "\n",
        "# transformando CSV em DataFrame\n",
        "duas_unas_df = pd.read_csv(duas_unas_dataset)  # Transformação em dataset\n",
        "\n",
        "duas_unas_validation = duas_unas_df.tail(validation_interval)  # Criação do dataset de validação\n",
        "\n",
        "indices_to_validate = duas_unas_df.index[-validation_interval:].tolist()  # Obtenção dos índices dos dadoss de validação\n",
        "duas_unas_df = duas_unas_df.drop(indices_to_validate).reset_index(drop=True)  # Retirada dos dados de validação do dataset original de treino/teste\n",
        "\n",
        "duas_unas_df['timestamp'] = pd.to_datetime(duas_unas_df['timestamp'])  # Transformação dos dados da coluna 'timestamp' no formato datetime. Ela lida com vários formatos de data/hora e tenta interpretá-los de forma inteligente.\n",
        "duas_unas_validation['timestamp'] = pd.to_datetime(duas_unas_validation['timestamp'])  # Idem da ação da linha de cima.\n",
        "\n",
        "duas_unas_df['index'] = range(len(duas_unas_df))  # Criação da coluna 'index' em que os valores vão de zero ao tamanho total do daframe\n",
        "duas_unas_validation['index'] = indices_to_validate  # Idem da acima da linha acima\n",
        "print(indices_to_validate)\n",
        "duas_unas_df['kwh'] = duas_unas_df['kWh fornecido']  # Criação da coluna 'kwh' que copia os dados da coluna 'kWh fornecido'\n",
        "duas_unas_validation['kwh'] = duas_unas_validation['kWh fornecido']  # Idem da acima da linha acima\n",
        "\n",
        "duas_unas_df.drop(columns=['timestamp'], inplace=True)  # Remoção da coluna 'timestamp', inclusive no dataframe original\n",
        "duas_unas_validation.drop(columns=['timestamp'], inplace=True)  # Idem da acima da linha acima\n",
        "\n",
        "duas_unas_df.drop(columns=['kWh fornecido'], inplace=True)  # Remoção da coluna 'kWh fornecido', inclusive no dataframe original\n",
        "duas_unas_validation.drop(columns=['kWh fornecido'], inplace=True)  # Idem da acima da linha acima\n",
        "\n",
        "X = duas_unas_df['index'].values.reshape(-1,1)  # Cria o array X com os índices do dataframe duas_unas_df\n",
        "y = duas_unas_df['kwh'].values  # Cria o array y com os dados de energia de duas_unas_df\n",
        "#print(y)\n",
        "X_validation = duas_unas_validation['index'].values.reshape(-1,1)  # Cria o array X_validation com os índices do dataframe duas_unas_validation\n",
        "y_validation = duas_unas_validation['kwh'].values  # Cria o array y com os dados de energia de duas_unas_validation\n",
        "#print(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# regr = MLPRegressor(hidden_layer_sizes=(50,50,50), max_iter=200).fit(X_train, y_train)\n",
        "\n",
        "# y_pred = regr.predict(X_test)\n",
        "\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# rms = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_test, y_test, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.scatter(X_test, y_pred, color=\"red\", label=\"Previsões\", alpha=0.6)\n",
        "# plt.title(f\"Teste do MLPRegressor (RMSE: {rms:.2f})\")\n",
        "# plt.xlabel(\"Horas (X)\")\n",
        "# plt.ylabel(\"Previsão (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(X_validation)\n",
        "# y_pred_validation = regr.predict(X_validation)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_validation, y_validation, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.scatter(X_validation, y_pred_validation, color=\"red\", label=\"Previsões\", alpha=0.6)\n",
        "# plt.title(f\"Hora da Verdade\")\n",
        "# plt.xlabel(\"Hora (X)\")\n",
        "# plt.ylabel(\"Consumo (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pirapama_dataset = f'{base_path}outras_unidades/pirapama/agrupado_hora_Pirapama_2021.csv'\n",
        "# # transformando CSV em DataFrame\n",
        "# pirapama_df = pd.read_csv(pirapama_dataset)\n",
        "\n",
        "# pirapama_df['Data'] = pd.to_datetime(pirapama_df['Data'])\n",
        "\n",
        "# pirapama_df['index'] = range(len(pirapama_df))\n",
        "\n",
        "# pirapama_df['kwh'] = pirapama_df['kWh fornecido'] \n",
        "\n",
        "# pirapama_df.drop(columns=['Data'], inplace=True)\n",
        "\n",
        "# pirapama_df.drop(columns=['kWh fornecido'], inplace=True)\n",
        "\n",
        "# X_pirapama = pirapama_df['index'].values.reshape(-1,1)\n",
        "# y_pirapama = pirapama_df['kwh'].values\n",
        "\n",
        "# y_pred = regr.predict(X_pirapama)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.plot(X_pirapama, y_pirapama, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.plot(X_pirapama, y_pred, color=\"red\", label=\"Valores Previstos\")\n",
        "# plt.title(f\"Hora da Verdade\")\n",
        "# plt.xlabel(\"Hora (X)\")\n",
        "# plt.ylabel(\"Consumo (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Renan Torres\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Padronização dos dados\n",
        "scaled_kwh = scaler.fit_transform(duas_unas_df['kwh'].values.reshape(-1, 1))  # padronização de duas_unas_df\n",
        "\n",
        "scaled_kwh_validation = scaler.fit_transform(duas_unas_validation['kwh'].values.reshape(-1,1))  # padronização de duas_unas_validacao\n",
        "\n",
        "def create_dataset(data, time_step, output_steps):  # Função para construção do dataset\n",
        "    X, y = [], []  # Criação das listas\n",
        "    for i in range(len(data) - time_step - output_steps + 1):  # Loop\n",
        "        X.append(data[i:(i + time_step), 0])  # Construção do dataset \n",
        "        y.append(data[(i + time_step):(i + time_step + output_steps), 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset\n",
        "X, y = create_dataset(scaled_kwh, time_step, output_steps)\n",
        "\n",
        "X_validation, y_validation = create_dataset(scaled_kwh_validation, time_step, output_steps)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(10, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(10, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(10, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(output_steps))\n",
        "\n",
        "# learning_rate = 0.0005\n",
        "# optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.02, patience=2, verbose=0, mode='auto')\n",
        "# history = model.fit(X_train, y_train, epochs=100000, batch_size=8, validation_split=.2, callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=2, patience=3))\n",
        "\n",
        "# # Train the model\n",
        "# #history = model.fit(X_train, y_train, epochs=1000, batch_size=4, verbose=1, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1))\n",
        "\n",
        "# # Plotar as perdas de treinamento e validação\n",
        "# plt.plot(history.history['loss'], label='Loss de Treinamento')\n",
        "# plt.plot(history.history['val_loss'], label='Loss de Validação')\n",
        "# plt.xlabel('Épocas')\n",
        "# plt.ylabel('Perda')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_hyperparameters(params):\n",
        "    num_camadas_LSTM, learning_rate, batch_size, patience, epochs, loss, optimizer, units, dropout_rate, activation = params\n",
        "    \n",
        "    # Criar o modelo\n",
        "    model = Sequential()\n",
        "    for _ in range(int(num_camadas_LSTM)):\n",
        "        model.add(LSTM(int(units), return_sequences=True if _ < int(num_camadas_LSTM) - 1 else False, input_shape=(X_train.shape[1], 1)))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(output_steps, activation=activation))\n",
        "\n",
        "    # Selecionar o otimizador\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:  # 'sgd'\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "\n",
        "    # Compilar o modelo\n",
        "    model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "    # Treinar o modelo com EarlyStopping\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=int(patience), restore_best_weights=True)\n",
        "    \n",
        "    history = model.fit(X_train, y_train, batch_size=int(batch_size), epochs=int(epochs), validation_data=(X_validation, y_validation), \n",
        "                        verbose=0, callbacks=[early_stopping])\n",
        "    \n",
        "    \n",
        "    # Plotar as perdas de treinamento e validação\n",
        "    plt.plot(history.history['loss'], label='Loss de Treinamento')\n",
        "    plt.plot(history.history['val_loss'], label='Loss de Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    # Avaliar o modelo\n",
        "    score = model.evaluate(X_validation, y_validation, verbose=0)\n",
        "    print(f\"score: {score}\")\n",
        "    \n",
        "    return score  # Retorna a perda a ser minimizada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m ub \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# limites superiores\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Executando o PSO\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mpso\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped_evaluate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhores Hiperparâmetros:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhor Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_score)\n",
            "File \u001b[1;32mc:\\Users\\Renan Torres\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyswarm\\pso.py:111\u001b[0m, in \u001b[0;36mpso\u001b[1;34m(func, lb, ub, ieqcons, f_ieqcons, args, kwargs, swarmsize, omega, phip, phig, maxiter, minstep, minfunc, debug)\u001b[0m\n\u001b[0;32m    108\u001b[0m p[i, :] \u001b[38;5;241m=\u001b[39m x[i, :]\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Calculate the objective's value at the current particle's\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m fp[i] \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# At the start, there may not be any feasible starting point, so just\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# give it a temporary \"best\" point since it's likely to change\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Renan Torres\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyswarm\\pso.py:74\u001b[0m, in \u001b[0;36mpso.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     71\u001b[0m vlow \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mvhigh\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Check for constraint function(s) #########################################\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f_ieqcons \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ieqcons):\n",
            "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mmapped_evaluate\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Criar uma lista de parâmetros, substituindo os índices de perda, otimizador e ativação por suas strings correspondentes\u001b[39;00m\n\u001b[0;32m     15\u001b[0m modified_params \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     params[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# num_camadas_LSTM\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     params[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# learning_rate\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     activation_options[activation_idx]  \u001b[38;5;66;03m# função de ativação\u001b[39;00m\n\u001b[0;32m     26\u001b[0m ]\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mevaluate_hyperparameters\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m     31\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(patience), restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 33\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(batch_size), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(epochs), validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mX_val\u001b[49m, y_val), \n\u001b[0;32m     34\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Plotar as perdas de treinamento e validação\u001b[39;00m\n\u001b[0;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss de Treinamento\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ],
      "source": [
        "from pyswarm import pso\n",
        "\n",
        "def mapped_evaluate(params):\n",
        "    # Mapeamentos para as opções de perda, otimizadores e ativação\n",
        "    loss_options = [\"mean_squared_error\", \"mean_absolute_error\"]\n",
        "    optimizer_options = [\"adam\", \"rmsprop\", \"sgd\"]\n",
        "    activation_options = [\"tanh\", \"relu\", \"sigmoid\"]\n",
        "\n",
        "    # Mapear índices para os parâmetros\n",
        "    loss_idx = int(params[5])  # índice da função de perda\n",
        "    optimizer_idx = int(params[6])  # índice do otimizador\n",
        "    activation_idx = int(params[9])  # índice da função de ativação\n",
        "\n",
        "    # Criar uma lista de parâmetros, substituindo os índices de perda, otimizador e ativação por suas strings correspondentes\n",
        "    modified_params = [\n",
        "        params[0],  # num_camadas_LSTM\n",
        "        params[1],  # learning_rate\n",
        "        params[2],  # batch_size\n",
        "        params[3],  # patience\n",
        "        params[4],  # epochs\n",
        "        loss_options[loss_idx],  # função de perda\n",
        "        optimizer_options[optimizer_idx],  # otimizador\n",
        "        params[7],  # units\n",
        "        params[8],  # dropout_rate\n",
        "        activation_options[activation_idx]  # função de ativação\n",
        "    ]\n",
        "\n",
        "    return evaluate_hyperparameters(modified_params)\n",
        "\n",
        "# Limites para os hiperparâmetros\n",
        "lb = [1, 1e-5, 8, 2, 30, 0, 0, 10, 0.1, 0]  # limites inferiores\n",
        "ub = [5, 1e-2, 64, 10, 500, 1, 2, 200, 0.5, 2]  # limites superiores\n",
        "\n",
        "# Executando o PSO\n",
        "best_params, best_score = pso(mapped_evaluate, lb, ub)\n",
        "\n",
        "print(\"Melhores Hiperparâmetros:\", best_params)\n",
        "print(\"Melhor Score:\", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def avaliacao_multistep(janela_real, janela_prevista):\n",
        "    \n",
        "    mae_med = 0\n",
        "    mse_med = 0\n",
        "    rmse_med = 0\n",
        "    mape_med = 0\n",
        "    acuracia_med = 0\n",
        "    # Converte as listas para arrays NumPy\n",
        "    #janela_real = np.array(janela_real)\n",
        "    #janela_prevista = np.array(janela_prevista)\n",
        "    print(len(janela_real[0]))\n",
        "    \n",
        "    for j in range(len(janela_prevista)):\n",
        "        if len(janela_real) != len(janela_prevista):\n",
        "            raise ValueError(\"As janelas reais e previstas devem ter o mesmo comprimento.\")\n",
        "\n",
        "        else:\n",
        "        #r2 =  round(r2_score(janela_real, janela_prevista), 2)   # Relacionado à regressão linear dos valores verdadeiros. É calculado com base no erro quadrático residual e erro total. O valor pode ser interpretado como a proporção dos valores previstos que estão na linha criada pela regressão.\n",
        "        #print(\"R^2 Score of LSTM model = \",r2)\n",
        "        #print(\" \")\n",
        "\n",
        "            mae = round(mean_absolute_error(janela_real[j], janela_prevista[j]), 3) # Arredondamento com três casas decimais\n",
        "            print(\"MAE(Erro absoluto médio) Score of LSTM model = \",mae)\n",
        "            print(\" \")\n",
        "\n",
        "            mse = round(mean_squared_error(janela_real[j], janela_prevista[j]), 3)\n",
        "            print(\"MSE(Erro quadrático médio) Score of LSTM model = \",mse)\n",
        "            print(\" \")\n",
        "            \n",
        "            rmse = round(np.sqrt(mse), 3)\n",
        "            print(\"RMSE(Raiz do erro quadrático médio) Score of LSTM model = \",rmse)\n",
        "            print(\" \")\n",
        "\n",
        "            mape = round(mean_absolute_percentage_error(janela_real[j], janela_prevista[j]), 2)\n",
        "            print(\"MAPE(Erro percentual médio absoluto) Score of LSTM model = \",mape)\n",
        "            print(\" \")\n",
        "            \n",
        "            ### Cálculo da ACURÁCIA ###\n",
        "            \n",
        "            # Calcula a diferença percentual\n",
        "            diferencas_percentuais = np.abs((np.array(janela_real[j]) - np.array(janela_prevista[j])) / np.array(janela_real[j]))\n",
        "\n",
        "            # Conta quantas diferenças percentuais estão dentro da tolerância\n",
        "            acertos = np.sum(diferencas_percentuais <= 0.05) #0.05 é a tolerância\n",
        "\n",
        "            # Calcula a acurácia\n",
        "            acuracia = acertos / len(janela_real[0])   \n",
        "            acuracia = round(100 * acuracia, 2)\n",
        "            \n",
        "            print(\"Accuracy Score of LSTM model = \",acuracia)\n",
        "            mae_med += mae\n",
        "            mse_med += mse\n",
        "            rmse_med += rmse\n",
        "            mape_med += mape\n",
        "            acuracia_med += acuracia\n",
        "            \n",
        "    mae_med = mae_med/(len(janela_prevista))\n",
        "    mse_med /=(len(janela_prevista))\n",
        "    rmse_med /=(len(janela_prevista))\n",
        "    mape_med /=(len(janela_prevista))\n",
        "    acuracia_med /=(len(janela_prevista))\n",
        "    \n",
        "    return round(mae_med,2), round(mse_med,2), round(rmse_med,2), round(mape_med,2), round(acuracia_med,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_validation)\n",
        "print(len(X_validation), len(y_pred), len(X_validation[0]))\n",
        "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_rescaled = scaler.inverse_transform(y_validation)\n",
        "#indices_to_validate = indices_to_validate[:len(y_pred_rescaled)]\n",
        "indices_to_validate = indices_to_validate[:validation_interval]\n",
        "print(indices_to_validate)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[0:output_steps], y_test_rescaled[0], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[0:output_steps], y_pred_rescaled[0], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,80000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[3:3+output_steps], y_test_rescaled[3], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[3:3+output_steps], y_pred_rescaled[3], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,80000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[6:6+output_steps], y_test_rescaled[6], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[6:6+output_steps], y_pred_rescaled[6], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,80000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[9:9+output_steps], y_test_rescaled[9], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[9:9+output_steps], y_pred_rescaled[9], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,80000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[11:11+output_steps], y_test_rescaled[11], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[11:11+output_steps], y_pred_rescaled[11], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,80000,step=10000))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avaliacao_multistep(y_test_rescaled,y_pred_rescaled)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rQy75p6HhZim",
        "qW4ExC6UnZz_",
        "sfY3t1p7oHSV",
        "E1yXXSWtDxuN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
