{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0181 \n",
      "Epoch 1/32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, input_shape, units=50, dropout_rate=0.2, \n",
    "                 num_layers=1, activation='tanh'):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            return_sequences = i < num_layers - 1  # Only the last layer should not return sequences\n",
    "            if i == 0:\n",
    "                self.model.add(LSTM(units, return_sequences=return_sequences, \n",
    "                                    activation=activation, input_shape=input_shape))\n",
    "            else:\n",
    "                self.model.add(LSTM(units, return_sequences=return_sequences, \n",
    "                                    activation=activation))\n",
    "            self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "        self.model.add(Dense(output_steps))  # Final dense layer for output\n",
    "\n",
    "    def compile(self, optimizer='adam', learning_rate=0.0005):\n",
    "        \"\"\"\n",
    "        Compiles the model with the specified optimizer and learning rate.\n",
    "        \n",
    "        Parameters:\n",
    "        - optimizer: Type of optimizer to use ('adam', 'rmsprop', 'sgd').\n",
    "        - learning_rate: Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        if optimizer == 'adam':\n",
    "            optimizer_instance = Adam(learning_rate=learning_rate)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            optimizer_instance = RMSprop(learning_rate=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            optimizer_instance = SGD(learning_rate=learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        self.model.compile(optimizer=optimizer_instance, loss='mean_squared_error')\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=100, batch_size=32, patience=5, is_validation=False):\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True)\n",
    "        trained_model = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                       validation_split=.2, callbacks=[early_stopping])\n",
    "\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)  # Update output without scrolling down\n",
    "        \n",
    "        return trained_model.history['loss'], trained_model.history['val_loss']\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        mse = self.model.evaluate(X_test, y_test)\n",
    "        return mse \n",
    "        \n",
    "\n",
    "class PSOHyperparameterOptimization:\n",
    "    def __init__(self, model_class, param_bounds, n_particles=30, n_iterations=50):\n",
    "        self.model_class = model_class\n",
    "        self.param_bounds = param_bounds\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.best_scores = []\n",
    "\n",
    "    def optimize(self, X_train, y_train, input_shape):\n",
    "        for i in range(self.n_iterations):\n",
    "            # Sample parameters for each particle\n",
    "            params = {\n",
    "                'input_shape': input_shape,  # Include the input shape for the model\n",
    "                'units': int(np.random.uniform(*self.param_bounds['units'])),  # Number of LSTM units\n",
    "                'dropout_rate': np.random.uniform(*self.param_bounds['dropout_rate']),  # Dropout rate\n",
    "                'num_layers': int(np.random.uniform(*self.param_bounds['num_layers'])),  # Number of LSTM layers\n",
    "                'activation': np.random.choice(self.param_bounds['activation']),  # Activation function\n",
    "                'epochs': np.random.choice(self.param_bounds['epochs']),\n",
    "                'optimizer': np.random.choice(self.param_bounds['optimizer']),\n",
    "                'learning_rate': np.random.uniform(*self.param_bounds['learning_rate']),\n",
    "\n",
    "            }\n",
    "\n",
    "            # Create and compile the model\n",
    "            model = self.model_class(input_shape, units=params['units'], dropout_rate=params['dropout_rate'], \n",
    "                 num_layers=params['num_layers'], activation=params['activation'])\n",
    "            # Compile the model with the specific optimizer and learning rate\n",
    "            model.compile(optimizer=params['optimizer'], learning_rate=params['learning_rate'])\n",
    "            # Fit the model with the specific parameters\n",
    "            model.fit(X_train, y_train, epochs=int(np.random.uniform(*self.param_bounds['epochs'])), batch_size=int(np.random.uniform(*self.param_bounds['batch_size'])), \n",
    "                      patience=int(np.random.uniform(*self.param_bounds['patience'])))\n",
    "\n",
    "            # Evaluate the model\n",
    "            score = model.evaluate(X_train, y_train)\n",
    "            params['score'] = score\n",
    "\n",
    "            self.best_scores.append(params)\n",
    "        \n",
    "            self.best_scores = sorted(self.best_scores, key=lambda x: x['score'])            \n",
    "\n",
    "            if len(self.best_scores) > 5:\n",
    "                self.best_scores.pop()\n",
    "\n",
    "        return self.best_scores\n",
    "\n",
    "\n",
    "base_path = './dataset/'\n",
    "\n",
    "# carregando arquivo CSV\n",
    "time_base = 'days'  # Escolha do tipo de agrupamento\n",
    "duas_unas_dataset = f'{base_path}duas_unas/{time_base}/grouped_1_{time_base}.csv'  # Definição do caminho do diretório do dataset\n",
    "print(len(duas_unas_dataset))\n",
    "validation_interval = 97  # Definição da parcela dos dados que serão retiradas do treinamento/teste para validação\n",
    "time_step = 1  # Number of previous time steps to consider\n",
    "output_steps = 15  # Number of future steps to predict\n",
    "\n",
    "# transformando CSV em DataFrame\n",
    "duas_unas_df = pd.read_csv(duas_unas_dataset)  # Transformação em dataset\n",
    "\n",
    "duas_unas_validation = duas_unas_df.tail(validation_interval)  # Criação do dataset de validação\n",
    "\n",
    "indices_to_validate = duas_unas_df.index[-validation_interval:].tolist()  # Obtenção dos índices dos dadoss de validação\n",
    "duas_unas_df = duas_unas_df.drop(indices_to_validate).reset_index(drop=True)  # Retirada dos dados de validação do dataset original de treino/teste\n",
    "\n",
    "duas_unas_df['timestamp'] = pd.to_datetime(duas_unas_df['timestamp'])  # Transformação dos dados da coluna 'timestamp' no formato datetime. Ela lida com vários formatos de data/hora e tenta interpretá-los de forma inteligente.\n",
    "duas_unas_validation['timestamp'] = pd.to_datetime(duas_unas_validation['timestamp'])  # Idem da ação da linha de cima.\n",
    "\n",
    "duas_unas_df['index'] = range(len(duas_unas_df))  # Criação da coluna 'index' em que os valores vão de zero ao tamanho total do daframe\n",
    "duas_unas_validation['index'] = indices_to_validate  # Idem da acima da linha acima\n",
    "print(indices_to_validate)\n",
    "duas_unas_df['kwh'] = duas_unas_df['kWh fornecido']  # Criação da coluna 'kwh' que copia os dados da coluna 'kWh fornecido'\n",
    "duas_unas_validation['kwh'] = duas_unas_validation['kWh fornecido']  # Idem da acima da linha acima\n",
    "\n",
    "duas_unas_df.drop(columns=['timestamp'], inplace=True)  # Remoção da coluna 'timestamp', inclusive no dataframe original\n",
    "duas_unas_validation.drop(columns=['timestamp'], inplace=True)  # Idem da acima da linha acima\n",
    "\n",
    "duas_unas_df.drop(columns=['kWh fornecido'], inplace=True)  # Remoção da coluna 'kWh fornecido', inclusive no dataframe original\n",
    "duas_unas_validation.drop(columns=['kWh fornecido'], inplace=True)  # Idem da acima da linha acima\n",
    "\n",
    "X = duas_unas_df['index'].values.reshape(-1,1)  # Cria o array X com os índices do dataframe duas_unas_df\n",
    "y = duas_unas_df['kwh'].values  # Cria o array y com os dados de energia de duas_unas_df\n",
    "\n",
    "X_validation = duas_unas_validation['index'].values.reshape(-1,1)  # Cria o array X_validation com os índices do dataframe duas_unas_validation\n",
    "y_validation = duas_unas_validation['kwh'].values  # Cria o array y com os dados de energia de duas_unas_validation\n",
    "y_validation_plot = duas_unas_validation['kwh'].values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Padronização dos dados\n",
    "scaled_kwh = scaler.fit_transform(duas_unas_df['kwh'].values.reshape(-1, 1))  # padronização de duas_unas_df\n",
    "\n",
    "scaled_kwh_validation = scaler.fit_transform(duas_unas_validation['kwh'].values.reshape(-1,1))  # padronização de duas_unas_validacao\n",
    "\n",
    "def create_dataset(data, time_step, output_steps):  # Função para construção do dataset\n",
    "    X, y = [], []  # Criação das listas\n",
    "    for i in range(len(data) - time_step - output_steps + 1):  # Loop\n",
    "        X.append(data[i:(i + time_step), 0])  # Construção do dataset \n",
    "        y.append(data[(i + time_step):(i + time_step + output_steps), 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create the dataset\n",
    "X, y = create_dataset(scaled_kwh, time_step, output_steps)\n",
    "\n",
    "X_validation, y_validation = create_dataset(scaled_kwh_validation, time_step, output_steps)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "for i in range(0,100000,100):\n",
    "    if i is not 0:\n",
    "        pso_lst_params = {\n",
    "            'units': (10, 200),                   \n",
    "            'dropout_rate': (0.1, 0.5),         \n",
    "            'learning_rate': (1e-5, 1e-2),      \n",
    "            'optimizer': [\"adam\", \"rmsprop\", \"sgd\"],  \n",
    "            'activation': [\"tanh\", \"relu\", \"sigmoid\"],  \n",
    "            'num_layers': (1, 10),                 \n",
    "            'epochs': (30, 100),\n",
    "            'batch_size': (8, 64), \n",
    "            'patience': (2, 10),\n",
    "            'n_iterations': i,\n",
    "            'n_particles': 30\n",
    "        }\n",
    "\n",
    "        output_dir = f'outputs/pso-iterations/#{pso_lst_params[\"n_iterations\"]}_iteration/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Instantiate PSO for hyperparameter optimization\n",
    "        pso_optimizer = PSOHyperparameterOptimization(LSTMModel, n_iterations=pso_lst_params['n_iterations'], n_particles=pso_lst_params['n_particles'], param_bounds=pso_lst_params)\n",
    "\n",
    "        # Perform optimization (input_shape should match your data's input dimensions)\n",
    "        best_scores_ranking = pso_optimizer.optimize(X_train, y_train, input_shape=(X_train.shape[1], 1))\n",
    "\n",
    "        for (i, best_params) in enumerate(best_scores_ranking):\n",
    "            final_model = LSTMModel(input_shape=best_params['input_shape'], units=best_params['units'], dropout_rate=best_params['dropout_rate'], \n",
    "                            num_layers=best_params['num_layers'], activation=best_params['activation'])\n",
    "            final_model.compile()\n",
    "            loss, val_loss = final_model.fit(X_train, y_train, epochs=best_params['epochs'], is_validation=True)\n",
    "\n",
    "            if (loss):\n",
    "                plt.figure()\n",
    "                plt.plot(loss, label='Training Loss')\n",
    "                plt.plot(val_loss, label='Validation Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.savefig(f'{output_dir}{round(best_params['score'],4)*100}-training.png')\n",
    "\n",
    "            predictions = final_model.predict(X_validation)\n",
    "\n",
    "            predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "\n",
    "            y_validation_plot = duas_unas_validation['kwh'].values\n",
    "\n",
    "            y_validation_plot = y_validation_plot[:len(predictions_rescaled)]\n",
    "\n",
    "            plt.figure(i,figsize=(14, 10))\n",
    "            plt.plot(y_validation_plot, label='Actual kWh', color='blue', linestyle='-', alpha=0.7)\n",
    "            plt.plot(predictions_rescaled[:,0], label='Predicted kWh', color='red', linestyle='--', alpha=0.7)\n",
    "            plt.xticks(np.arange(0,len(y_validation_plot), 2))\n",
    "            plt.title(f'Prediction vs Actual kWh - score {best_params['score']}')\n",
    "            plt.xlabel('Index')\n",
    "            plt.ylabel('kWh')\n",
    "            plt.legend()\n",
    "            plt.grid() \n",
    "            plt.savefig(f'{output_dir}{round(best_params['score'],4)*100}.png')\n",
    "            pd.DataFrame(best_params).to_json(f'{output_dir}{round(best_params['score'],4)*100}.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
