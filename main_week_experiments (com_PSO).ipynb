{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQy75p6HhZim"
      },
      "source": [
        "## **Importação das bibliotecas e dados (AEP e Duas Unas)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install ipykernel --break-system-packages\n",
        "# %pip install scikit-learn --break-system-packages\n",
        "# %pip install matplotlib --break-system-packages\n",
        "# %pip install keras --break-system-packages\n",
        "# %pip install tensorflow --break-system-packages\n",
        "# %pip install pandas --break-system-packages\n",
        "# %pip install plotly --break-system-packages\n",
        "# %pip install seaborn --break-system-packages\n",
        "# %pip install nbformat --break-system-packages\n",
        "# %pip install keras --break-system-packages\n",
        "# %pip install pswarm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.initializers import GlorotUniform    #Inicializador de pesos\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf #Biblioteca para machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D-eSfJgl8qO_"
      },
      "outputs": [],
      "source": [
        "\n",
        "base_path = './dataset/'\n",
        "\n",
        "# carregando arquivo CSV\n",
        "time_base = 'days'  # Escolha do tipo de agrupamento\n",
        "duas_unas_dataset = f'{base_path}duas_unas/{time_base}/grouped_1_{time_base}.csv'  # Definição do caminho do diretório do dataset\n",
        "print(len(duas_unas_dataset))\n",
        "validation_interval = 61  # Definição da parcela dos dados que serão retiradas do treinamento/teste para validação\n",
        "time_step = 1  # Number of previous time steps to consider\n",
        "output_steps = 60  # Number of future steps to predict\n",
        "\n",
        "# transformando CSV em DataFrame\n",
        "duas_unas_df = pd.read_csv(duas_unas_dataset)  # Transformação em dataset\n",
        "\n",
        "duas_unas_validation = duas_unas_df.tail(validation_interval)  # Criação do dataset de validação\n",
        "\n",
        "indices_to_validate = duas_unas_df.index[-validation_interval:].tolist()  # Obtenção dos índices dos dadoss de validação\n",
        "duas_unas_df = duas_unas_df.drop(indices_to_validate).reset_index(drop=True)  # Retirada dos dados de validação do dataset original de treino/teste\n",
        "\n",
        "duas_unas_df['timestamp'] = pd.to_datetime(duas_unas_df['timestamp'])  # Transformação dos dados da coluna 'timestamp' no formato datetime. Ela lida com vários formatos de data/hora e tenta interpretá-los de forma inteligente.\n",
        "duas_unas_validation['timestamp'] = pd.to_datetime(duas_unas_validation['timestamp'])  # Idem da ação da linha de cima.\n",
        "\n",
        "duas_unas_df['index'] = range(len(duas_unas_df))  # Criação da coluna 'index' em que os valores vão de zero ao tamanho total do daframe\n",
        "duas_unas_validation['index'] = indices_to_validate  # Idem da acima da linha acima\n",
        "print(indices_to_validate)\n",
        "duas_unas_df['kwh'] = duas_unas_df['kWh fornecido']  # Criação da coluna 'kwh' que copia os dados da coluna 'kWh fornecido'\n",
        "duas_unas_validation['kwh'] = duas_unas_validation['kWh fornecido']  # Idem da acima da linha acima\n",
        "\n",
        "duas_unas_df.drop(columns=['timestamp'], inplace=True)  # Remoção da coluna 'timestamp', inclusive no dataframe original\n",
        "duas_unas_validation.drop(columns=['timestamp'], inplace=True)  # Idem da acima da linha acima\n",
        "\n",
        "duas_unas_df.drop(columns=['kWh fornecido'], inplace=True)  # Remoção da coluna 'kWh fornecido', inclusive no dataframe original\n",
        "duas_unas_validation.drop(columns=['kWh fornecido'], inplace=True)  # Idem da acima da linha acima\n",
        "\n",
        "X = duas_unas_df['index'].values.reshape(-1,1)  # Cria o array X com os índices do dataframe duas_unas_df\n",
        "y = duas_unas_df['kwh'].values  # Cria o array y com os dados de energia de duas_unas_df\n",
        "#print(y)\n",
        "X_validation = duas_unas_validation['index'].values.reshape(-1,1)  # Cria o array X_validation com os índices do dataframe duas_unas_validation\n",
        "y_validation = duas_unas_validation['kwh'].values  # Cria o array y com os dados de energia de duas_unas_validation\n",
        "#print(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# regr = MLPRegressor(hidden_layer_sizes=(50,50,50), max_iter=200).fit(X_train, y_train)\n",
        "\n",
        "# y_pred = regr.predict(X_test)\n",
        "\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# rms = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_test, y_test, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.scatter(X_test, y_pred, color=\"red\", label=\"Previsões\", alpha=0.6)\n",
        "# plt.title(f\"Teste do MLPRegressor (RMSE: {rms:.2f})\")\n",
        "# plt.xlabel(\"Horas (X)\")\n",
        "# plt.ylabel(\"Previsão (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(X_validation)\n",
        "# y_pred_validation = regr.predict(X_validation)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_validation, y_validation, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.scatter(X_validation, y_pred_validation, color=\"red\", label=\"Previsões\", alpha=0.6)\n",
        "# plt.title(f\"Hora da Verdade\")\n",
        "# plt.xlabel(\"Hora (X)\")\n",
        "# plt.ylabel(\"Consumo (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pirapama_dataset = f'{base_path}outras_unidades/pirapama/agrupado_hora_Pirapama_2021.csv'\n",
        "# # transformando CSV em DataFrame\n",
        "# pirapama_df = pd.read_csv(pirapama_dataset)\n",
        "\n",
        "# pirapama_df['Data'] = pd.to_datetime(pirapama_df['Data'])\n",
        "\n",
        "# pirapama_df['index'] = range(len(pirapama_df))\n",
        "\n",
        "# pirapama_df['kwh'] = pirapama_df['kWh fornecido'] \n",
        "\n",
        "# pirapama_df.drop(columns=['Data'], inplace=True)\n",
        "\n",
        "# pirapama_df.drop(columns=['kWh fornecido'], inplace=True)\n",
        "\n",
        "# X_pirapama = pirapama_df['index'].values.reshape(-1,1)\n",
        "# y_pirapama = pirapama_df['kwh'].values\n",
        "\n",
        "# y_pred = regr.predict(X_pirapama)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.plot(X_pirapama, y_pirapama, color=\"blue\", label=\"Valores Reais\")\n",
        "# plt.plot(X_pirapama, y_pred, color=\"red\", label=\"Valores Previstos\")\n",
        "# plt.title(f\"Hora da Verdade\")\n",
        "# plt.xlabel(\"Hora (X)\")\n",
        "# plt.ylabel(\"Consumo (y)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def identify_peaks_and_valleys(y_test):\n",
        "\n",
        "#     # Inicializa os pesos\n",
        "#     weights = tf.ones_like(y_test)\n",
        "\n",
        "#     # Obtém a forma do tensor\n",
        "#     length = tf.shape(y_test)[0]  # comprimento do vetor\n",
        "\n",
        "#     # Itera sobre o vetor para encontrar picos e vales\n",
        "#     for i in range(1, length - 1):\n",
        "#         if y_test[i] > y_test[i - 1] and y_test[i] > y_test[i + 1]:\n",
        "#             # Se é um pico\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[i]], [3.0])  # Aumenta o peso\n",
        "#         elif y_test[i] < y_test[i - 1] and y_test[i] < y_test[i + 1]:\n",
        "#             # Se é um vale\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[i]], [3.0])  # Aumenta o peso\n",
        "\n",
        "#     # Verificar o primeiro elemento\n",
        "#     if length >= 3:  # Verifica se existem elementos suficientes\n",
        "#         if (y_test[0] > y_test[1]) and (y_test[0] > y_test[2]):  # Se o primeiro valor for maior que o segundo e o terceiro, é um pico\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[0]], [3.0])  # Aumenta o peso\n",
        "#         elif (y_test[0] < y_test[1]) and (y_test[0] < y_test[2]):  # Se for menor, é um vale\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[0]], [3.0])  # Aumenta o peso\n",
        "\n",
        "#     # Verificar o último elemento\n",
        "#     if length >= 3:  # Verifica se existem elementos suficientes\n",
        "#         if (y_test[-1] > y_test[-2]) and (y_test[-1] > y_test[-3]):  # Se o último valor for maior que o penúltimo e o antepenúltimo, é um pico\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[length - 1]], [3.0])  # Aumenta o peso\n",
        "#         elif (y_test[-1] < y_test[-2]):  # Se for menor, é um vale\n",
        "#             weights = tf.tensor_scatter_nd_update(weights, [[length - 1]], [3.0])  # Aumenta o peso\n",
        "\n",
        "#     return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "### FUNÇÃO PARA IDENTIFICAR PICOS E VALES SEM AVALIAÇÃO DO PRIMEIRO E ÚLTIMO ELEMENTOS ###\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "# def identify_peaks_and_valleys(y_test):\n",
        "#     # Converte y_test em um tensor\n",
        "#     y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "#     # Inicializa pesos como um tensor de 1s do mesmo formato\n",
        "#     weights = tf.ones(tf.shape(y_test), dtype=tf.float32)\n",
        "\n",
        "#     # Obtenha o número de elementos\n",
        "#     num_elements = tf.shape(y_test)[0]\n",
        "\n",
        "#     # Obter os índices de picos e vales\n",
        "#     y_test_prev = tf.roll(y_test, shift=1, axis=0)\n",
        "#     y_test_next = tf.roll(y_test, shift=-1, axis=0)\n",
        "\n",
        "#     # Define picos e vales\n",
        "#     peaks = tf.where((y_test > y_test_prev) & (y_test > y_test_next))\n",
        "#     valleys = tf.where((y_test < y_test_prev) & (y_test < y_test_next))\n",
        "\n",
        "#     # Atualiza os pesos para picos e vales\n",
        "#     weights = tf.tensor_scatter_nd_update(weights, peaks, tf.fill(tf.shape(peaks)[:1], 3.0))\n",
        "#     weights = tf.tensor_scatter_nd_update(weights, valleys, tf.fill(tf.shape(valleys)[:1], 3.0))\n",
        "\n",
        "#     return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def identify_peaks_and_valleys(y_test):\n",
        "    # Converte y_test em um tensor\n",
        "    #y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "    # Inicializa pesos como um tensor de 1s do mesmo formato\n",
        "    weights = tf.ones(tf.shape(y_test), dtype=tf.float32)\n",
        "\n",
        "    # Condicional para verificar se y_test tem pelo menos 3 elementos\n",
        "    mask = tf.size(y_test) < 3\n",
        "    #print(tf.size(y_test) < 3)\n",
        "\n",
        "    # Função para processar picos e vales\n",
        "    def process_peaks_and_valleys():\n",
        "        # Certifique-se de que y_test é do tipo float32\n",
        "        y_test_float = tf.cast(y_test, tf.float32)\n",
        "\n",
        "        # Obtém os índices de picos e vales para elementos intermediários\n",
        "        # Corrigido: use tf.expand_dims para garantir que o primeiro elemento tenha o formato correto\n",
        "        y_test_prev = tf.concat([tf.expand_dims(y_test_float[0], axis=0), y_test_float[:-1]], axis=0)  # Adiciona o primeiro elemento como anterior\n",
        "        y_test_next = tf.concat([y_test_float[1:], tf.expand_dims(y_test_float[-1], axis=0)], axis=0)  # Adiciona o último elemento como próximo\n",
        "\n",
        "        # Define picos e vales para os elementos intermediários\n",
        "        peaks = tf.where((y_test_float > y_test_prev) & (y_test_float > y_test_next))\n",
        "        valleys = tf.where((y_test_float < y_test_prev) & (y_test_float < y_test_next))\n",
        "        # print(f\"picos\",peaks)\n",
        "        # print(f\"vales\",valleys)\n",
        "        # Atualiza os pesos para picos e vales intermediários\n",
        "        weights_updated = tf.tensor_scatter_nd_update(weights, peaks, tf.fill(tf.shape(peaks)[:1], 2.0))\n",
        "        weights_updated = tf.tensor_scatter_nd_update(weights_updated, valleys, tf.fill(tf.shape(valleys)[:1], 2.0))\n",
        "\n",
        "    #     # Verificação para o primeiro elemento\n",
        "    #     first_idx = 0\n",
        "    #     first_elem = tf.gather(y_test, first_idx)\n",
        "    #     second_elem = tf.gather(y_test, first_idx + 1)\n",
        "    #     third_elem = tf.gather(y_test, first_idx + 2)\n",
        "\n",
        "    #     # Atualiza o peso do primeiro elemento\n",
        "    #     first_weight_update = tf.where(\n",
        "    #         (first_elem < second_elem) & (first_elem < third_elem) |\n",
        "    #         (first_elem > second_elem) & (first_elem > third_elem),\n",
        "    #         3.0,\n",
        "    #         weights_updated[first_idx]\n",
        "    #     )\n",
        "    #     weights_updated = tf.tensor_scatter_nd_update(weights_updated, [[first_idx]], [first_weight_update])\n",
        "\n",
        "\n",
        "    #     # Verificação para o último elemento\n",
        "    #     last_idx = tf.shape(y_test)[0] - 1\n",
        "    #     last_elem = tf.gather(y_test, last_idx)\n",
        "    #     second_last_elem = tf.gather(y_test, last_idx - 1)\n",
        "    #     third_last_elem = tf.gather(y_test, last_idx - 2)\n",
        "\n",
        "    # # Atualiza o peso do último elemento\n",
        "    #     last_weight_update = tf.where(\n",
        "    #         (last_elem < second_last_elem) & (last_elem < third_last_elem) |\n",
        "    #         (last_elem > second_last_elem) & (last_elem > third_last_elem),\n",
        "    #         3.0,\n",
        "    #         weights_updated[last_idx]\n",
        "    #     )\n",
        "    #     weights_updated = tf.tensor_scatter_nd_update(weights_updated, [[last_idx]], [last_weight_update])\n",
        "\n",
        "        return weights_updated\n",
        "\n",
        "    # Condicional para executar a função de processamento se y_test tiver 3 ou mais elementos\n",
        "    weights = tf.cond(mask, lambda: weights, process_peaks_and_valleys)\n",
        "\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função WMSE ajustada com pesos para picos e vales\n",
        "def wmse(y_test, y_pred):\n",
        "    # Convertendo y_test e y_pred para float32\n",
        "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Removendo dimensões desnecessárias\n",
        "    #y_test = tf.squeeze(y_test)\n",
        "    #y_pred = tf.squeeze(y_pred)\n",
        "\n",
        "    # Obtendo pesos\n",
        "    weights = identify_peaks_and_valleys(y_test)\n",
        "    #tf.print(\"Pesos (weights) no WMSE:\", weights)\n",
        "\n",
        "    # Calculando WMSE\n",
        "    wmse_value = (tf.reduce_sum(weights * tf.square(y_test - y_pred)) / tf.reduce_sum(weights))\n",
        "    return wmse_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função WMAE ajustada com pesos para picos e vales\n",
        "def wmae(y_test, y_pred):\n",
        "    # Convertendo y_test e y_pred para float32\n",
        "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "    #y_test = tf.squeeze(y_test)\n",
        "    #y_pred = tf.squeeze(y_pred)\n",
        "\n",
        "    weights = identify_peaks_and_valleys(y_test)\n",
        "    #tf.print(\"Pesos (weights) no WMAE:\", weights)\n",
        "\n",
        "    # Calculando WMAE\n",
        "    wmae_value = (tf.reduce_sum(weights * tf.abs(y_test - y_pred)) / tf.reduce_sum(weights))\n",
        "    return wmae_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(y_test[5])\n",
        "# weights = identify_peaks_and_valleys(y_test[5])\n",
        "# print(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Padronização dos dados\n",
        "scaled_kwh = scaler.fit_transform(duas_unas_df['kwh'].values.reshape(-1, 1))  # padronização de duas_unas_df\n",
        "\n",
        "scaled_kwh_validation = scaler.fit_transform(duas_unas_validation['kwh'].values.reshape(-1,1))  # padronização de duas_unas_validacao\n",
        "\n",
        "def create_dataset(data, time_step, output_steps):  # Função para construção do dataset\n",
        "    X, y = [], []  # Criação das listas\n",
        "    for i in range(len(data) - time_step - output_steps + 1):  # Loop\n",
        "        X.append(data[i:(i + time_step), 0])  # Construção do dataset \n",
        "        y.append(data[(i + time_step):(i + time_step + output_steps), 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset\n",
        "X, y = create_dataset(scaled_kwh, time_step, output_steps)\n",
        "\n",
        "X_validation, y_validation = create_dataset(scaled_kwh_validation, time_step, output_steps)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
        "\n",
        "# # Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# print(y_test)\n",
        "# print(X_test.shape)\n",
        "\n",
        "# # Build the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "# model.add(Dropout(0.2))\n",
        "# #model.add(LSTM(10, return_sequences=True))\n",
        "# #model.add(Dropout(0.2))\n",
        "# model.add(LSTM(20, return_sequences=False))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(output_steps))\n",
        "\n",
        "# learning_rate = 0.0005\n",
        "# optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.02, patience=2, verbose=0, mode='auto')\n",
        "# history = model.fit(X_train, y_train, epochs=100000, batch_size=4, validation_split=.2, callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=2, patience=3))\n",
        "\n",
        "# # Train the model\n",
        "# #history = model.fit(X_train, y_train, epochs=1000, batch_size=4, verbose=1, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1))\n",
        "\n",
        "# # Plotar as perdas de treinamento e validação\n",
        "# plt.plot(history.history['loss'], label='Loss de Treinamento')\n",
        "# plt.plot(history.history['val_loss'], label='Loss de Validação')\n",
        "# plt.xlabel('Épocas')\n",
        "# plt.ylabel('Perda')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.activations import relu, tanh, sigmoid\n",
        "\n",
        "# Treina o modelo LSTM com os hiperparâmetros fornecidos pela partícula do PSO e avalia o modelo\n",
        "def fitness_function(particle):\n",
        "    units = int(particle[0])\n",
        "    dropout_rate = particle[1]\n",
        "    learning_rate = particle[2]\n",
        "    batch_size = int(particle[3])\n",
        "    #epochs = int(particle[4])\n",
        "    patience = int(particle[4])\n",
        "    num_camadas_LSTM = int(particle[5])\n",
        "    activation_idx = 1 #int(particle[6])\n",
        "    loss_idx = 1 #int(particle[7])\n",
        "    optimizer_idx = 1 #int(particle[8])\n",
        "    \n",
        "    print(f\"units\", units)\n",
        "    print(f\"dropout_rate\", dropout_rate)\n",
        "    print(f\"learning_rate\", learning_rate)\n",
        "    print(f\"batch_size\", batch_size)\n",
        "    print(f\"patience\", patience)\n",
        "    print(f\"num_camadas_LSTM\", num_camadas_LSTM)\n",
        "\n",
        "    # Mapeamentos para as métricas de perda, otimizadores e ativação\n",
        "    loss_options = [wmse, wmae, \"mean_squared_error\", \"mean_absolute_error\"]\n",
        "    optimizer_options = [\"adam\", \"rmsprop\", \"sgd\"]\n",
        "    activation_options = [\"tanh\", \"relu\", \"sigmoid\"]\n",
        "\n",
        "    # Mapear índices para os parâmetros\n",
        "    loss = loss_options[loss_idx]  # índice da função de perda\n",
        "    optimizer = optimizer_options[optimizer_idx]  # índice do otimizador\n",
        "    activation = activation_options[activation_idx]  # índice da função de ativação\n",
        "\n",
        "    print(f\"activation\", activation)\n",
        "    print(f\"loss\", loss)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Criar o modelo\n",
        "    model = Sequential()\n",
        "\n",
        "    # Laço para adicionar o número de camadas LSTM e dropout especificado por num_camadas_LSTM\n",
        "    for _ in range(int(num_camadas_LSTM)):\n",
        "        model.add(LSTM(int(units), return_sequences=True if _ < int(num_camadas_LSTM) - 1 else False, input_shape=(X_train.shape[1], 1)))\n",
        "        # A relação condicional define se a camada deve retornar a sequência completa ou somente \n",
        "        # o último estado. Apenas a última camada LSTM não retorna sequência, porque é a camada final\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(output_steps, activation=activation))\n",
        "    # camada de saída em que \"out_steps\" define o número de passos futuros que serão previstos\n",
        "\n",
        "    # Selecionar o otimizador\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:  # 'sgd'\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=loss)  # Compilação do modelo definição da função de perda\n",
        "\n",
        "    # Treinamento do modelo salvamento de 'loss' e 'val_loss' para posterior plot\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=int(patience), restore_best_weights=True)\n",
        "    # EarlyStopping: Importa o callback EarlyStopping, que para o treinamento se a perda de validação\n",
        "    # (val_loss) não melhorar após \"patience\" épocas. O parâmetro \"restore_best_weights=True\" garante\n",
        "    # que os pesos do melhor modelo serão restaurados.\n",
        "    \n",
        "    \n",
        "    \n",
        "    # k_folds = 2\n",
        "    # shuffle = False\n",
        "    # kf = KFold(n_splits=min(k_folds, len(X_train)), shuffle=shuffle)\n",
        "    # for train_index, val_index in kf.split(X_train):\n",
        "    #     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    #     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    #     # X_train, X_val = X_train[train_index], X_train[val_index]\n",
        "    #     # y_train, y_val = y_train[train_index], y_train[val_index]\n",
        "    #     history = model.fit(X_train_fold, y_train_fold, epochs=int(100000), batch_size=int(batch_size), validation_data=(X_val_fold, y_val_fold), shuffle=shuffle, verbose=0, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "    history = model.fit(X_train, y_train, batch_size=int(batch_size), epochs=int(100000), validation_data=(X_validation, y_validation), \n",
        "                        verbose=0, callbacks=[early_stopping])\n",
        "    # Modelo é treinado até que a condição do EarlyStopping seja alcançada. O \"loss\" e \"val_loss\" são salvos\n",
        "    #em \"history\" para plot posterior.\n",
        "\n",
        "    # Plotar as perdas de treinamento e validação\n",
        "    plt.plot(history.history['loss'], label='Loss de Treinamento')\n",
        "    plt.plot(history.history['val_loss'], label='Loss de Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    # # Avaliar o modelo\n",
        "    # score = model.evaluate(X_validation, y_validation, verbose=0)\n",
        "    # print(f\"score: {score}\")\n",
        "\n",
        "    # # Avaliação do desempenho no conjunto de validação\n",
        "    # #y_pred = model.predict(X_validation) # Previsão\n",
        "    \n",
        "    return model.evaluate(X_validation, y_validation) # Cálculo do erro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definições para PSO\n",
        "n_particles = 15   # Número de partículas (conjunto-candidato de hiperparâmetros) otimizadas por iteração\n",
        "n_iterations = 30  # Número de vezes que o PSO otimiza as partículas\n",
        "w = 0.5            # Inércia - controla a influência da velocidade anterior da partícula\n",
        "c1 = 0.5           # Coeficiente cognitivo - controla a influência da melhor posição da partícula\n",
        "c2 = 0.5           # Coeficiente social - controla a influência da melhor posição global\n",
        "\n",
        "# Limites dos hiperparâmetros (neurônios, dropout, learning rate, batch size, patience,num_camadas_LSTM, activation, loss)\n",
        "bounds = [(10, 200), (0.1, 0.4), (0.00001, 0.005), (16, 64), (2, 8), (1, 5)] #, (0, 2), (0, 1)]\n",
        "bounds = [(0, 3)]\n",
        "\n",
        "# Inicialização das partículas\n",
        "particles = np.random.rand(n_particles, len(bounds))                                                     \n",
        "# Gera uma matriz de números aleatórios no intervalo [0,1). A matriz \n",
        "# tem dimensões n_particles - linhas e len(bounds) - colunas, sendo len(bounds)\n",
        "# o número de hiperparâmetros sendo otimizados.\n",
        "particles = particles * (np.array([ub - lb for lb, ub in bounds])) + np.array([lb for lb, ub in bounds]) \n",
        "# Cada linha de particles representa uma partícula, e cada coluna representa um valor correspondente a um hiperparâmetro.\n",
        "# O primeiro np.array cria um array que contém a diferença entre o limite superior (ub)\n",
        "# e o limite inferior (lb) de cada hiperparâmetro. Após isso, particles (definidas inicialmente entre [0,1))\n",
        "# são multiplicadas pela amplitude de cada intervalo.\n",
        "# O segundo np.array cria um array com os valores mínimos de cada hiperparâmetro. Esse array é somado ao primeiro array\n",
        "# para garantir que todas as partículas terão seus hiperparâmetros dentro dos limites estabelecidos. \n",
        "\n",
        "\n",
        "velocities = np.random.rand(n_particles, len(bounds)) # Define as velocidades iniciais das partículas de forma aleatória\n",
        "pbest_positions = particles.copy()                    # Guarda a melhor posição individual de cada partícula\n",
        "pbest_scores = [float('inf')] * n_particles  # Inicialização com um valor alto\n",
        "#pbest_scores = np.full(n_particles, np.inf)           # Inicializa os melhores scores individuais como infinito\n",
        "gbest_position = None                                 # A melhor posição global encontrada por todas as partículas (inicialmente None)\n",
        "gbest_score = np.inf                                  # O melhor score global, inicialmente infinito\n",
        "\n",
        "# Função para atualizar a posição e a velocidade das partículas\n",
        "# Ajusta as partículas para que se movam em direção à melhor solução encontrada até o momento, \n",
        "# tanto individualmente (pbest_positions) quanto globalmente (gbest_position).\n",
        "def update_particles(particles, velocities):\n",
        "    # particles - array com as posições atuais de todas as partículas\n",
        "    # velocities - array com as velocidades atuais de todas as partículas\n",
        "    for i in range(n_particles):            # Loop para atualizar todas as partículas\n",
        "        velocities[i] = w * velocities[i] + c1 * np.random.rand() * (pbest_positions[i] - particles[i]) + c2 * np.random.rand() * (gbest_position - particles[i])\n",
        "        # Atualiza a velocidade da partícula. W (inercia) multiplica a velocidade atual da partícula.\n",
        "        # C1 (componente cognitiva) multiplica a diferença entre a melhor posição e a posição atual (da partícula).\n",
        "        # np.random.rand() gera uma matriz de números aleatórios entre [0,1) para evitar que a partícula fique presa em uma trajetória fixa\n",
        "        # C2 - (componente social) multiplica a diferença entre a melhor posição considerando todas as partículas\n",
        "        # e a posição atual da partícula.        \n",
        "        particles[i] += velocities[i]\n",
        "        # A nova posição da partícula é atualizada somando a posição atual à nova velocidade\n",
        "        # Move a partícula em direção à sua nova posição no espaço de soluções\n",
        "        particles[i] = np.clip(particles[i], [lb for lb, ub in bounds], [ub for lb, ub in bounds])\n",
        "        # np.clip garante que cada partícula permaneça dentro dos limites definidos em bounds para cada hiperparâmetro\n",
        "        # np.clip(x, a, b) faz com que, se x for menor que a, ele se torne a, e se for maior que b, ele se torne b.\n",
        "\n",
        "# Otimização\n",
        "# O PSO irá atualizar as partículas e verificar as melhores soluções n_iterations vezes.\n",
        "for iteration in range(n_iterations):\n",
        "    # Para cada iteração todas as partículas são avaliadas: seu desempenho e ajuste de sua posição e velocidade\n",
        "    for i in range(n_particles):\n",
        "        score = fitness_function(particles[i])\n",
        "        # Função que treina e testa o modelo com os hiperparâmetros da partícula atual\n",
        "        # Cada partícula é avaliada através da sua posição/estado atual (valores-candidatos dos seus hiperparâmetros)\n",
        "        # É retornado o erro que avalia quão boa é a posição atual\n",
        "\n",
        "        if score < pbest_scores[i]:\n",
        "            pbest_scores[i] = score\n",
        "            pbest_positions[i] = particles[i].copy()\n",
        "        # Se o score for o melhor score pessoal da partícula, este valor e a posição (hiperparâmetros) são salvos\n",
        "        # copy() é usado para garantir que uma nova instância da posição seja armazenada, evitando modificações acidentais.\n",
        "\n",
        "        if score < gbest_score:\n",
        "            gbest_score = score\n",
        "            gbest_position = particles[i].copy()\n",
        "            print(f\"best_score_until_now\", gbest_score)\n",
        "            print(\"Melhor solução encontrada até agora:\", gbest_position)\n",
        "        # Se o score for o melhor score global de todas as partículas, este valor e a posição (hiperparâmetros) são salvos\n",
        "\n",
        "    update_particles(particles, velocities)\n",
        "    # Após a avaliação de todas as partículas e seus hiperparâmetros (dentro da mesma iteração), a função update_particles\n",
        "    # é chamada para atualizar as posições das partículas (hiperparâmetros) com base nos resultados da iteração atual \n",
        "\n",
        "print(\"Melhor solução encontrada:\", gbest_position)\n",
        "print(\"Melhor score:\", gbest_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Melhor solução encontrada:\", gbest_position)\n",
        "print(\"Melhor score:\", gbest_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def avaliacao_multistep(janela_real, janela_prevista):\n",
        "    \n",
        "    mae_med = 0\n",
        "    mse_med = 0\n",
        "    rmse_med = 0\n",
        "    mape_med = 0\n",
        "    acuracia_med = 0\n",
        "    # Converte as listas para arrays NumPy\n",
        "    #janela_real = np.array(janela_real)\n",
        "    #janela_prevista = np.array(janela_prevista)\n",
        "    print(len(janela_real[0]))\n",
        "    \n",
        "    for j in range(len(janela_prevista)):\n",
        "        if len(janela_real) != len(janela_prevista):\n",
        "            raise ValueError(\"As janelas reais e previstas devem ter o mesmo comprimento.\")\n",
        "\n",
        "        else:\n",
        "        #r2 =  round(r2_score(janela_real, janela_prevista), 2)   # Relacionado à regressão linear dos valores verdadeiros. É calculado com base no erro quadrático residual e erro total. O valor pode ser interpretado como a proporção dos valores previstos que estão na linha criada pela regressão.\n",
        "        #print(\"R^2 Score of LSTM model = \",r2)\n",
        "        #print(\" \")\n",
        "\n",
        "            mae = round(mean_absolute_error(janela_real[j], janela_prevista[j]), 3) # Arredondamento com três casas decimais\n",
        "            print(\"MAE(Erro absoluto médio) Score of LSTM model = \",mae)\n",
        "            print(\" \")\n",
        "\n",
        "            mse = round(mean_squared_error(janela_real[j], janela_prevista[j]), 3)\n",
        "            print(\"MSE(Erro quadrático médio) Score of LSTM model = \",mse)\n",
        "            print(\" \")\n",
        "            \n",
        "            rmse = round(np.sqrt(mse), 3)\n",
        "            print(\"RMSE(Raiz do erro quadrático médio) Score of LSTM model = \",rmse)\n",
        "            print(\" \")\n",
        "\n",
        "            mape = round(mean_absolute_percentage_error(janela_real[j], janela_prevista[j]), 2)\n",
        "            print(\"MAPE(Erro percentual médio absoluto) Score of LSTM model = \",mape)\n",
        "            print(\" \")\n",
        "            \n",
        "            ### Cálculo da ACURÁCIA ###\n",
        "            \n",
        "            # Calcula a diferença percentual\n",
        "            diferencas_percentuais = np.abs((np.array(janela_real[j]) - np.array(janela_prevista[j])) / np.array(janela_real[j]))\n",
        "\n",
        "            # Conta quantas diferenças percentuais estão dentro da tolerância\n",
        "            acertos = np.sum(diferencas_percentuais <= 0.05) #0.05 é a tolerância\n",
        "\n",
        "            # Calcula a acurácia\n",
        "            acuracia = acertos / len(janela_real[0])   \n",
        "            acuracia = round(100 * acuracia, 2)\n",
        "            \n",
        "            print(\"Accuracy Score of LSTM model = \",acuracia)\n",
        "            mae_med += mae\n",
        "            mse_med += mse\n",
        "            rmse_med += rmse\n",
        "            mape_med += mape\n",
        "            acuracia_med += acuracia\n",
        "            \n",
        "    mae_med = mae_med/(len(janela_prevista))\n",
        "    mse_med /=(len(janela_prevista))\n",
        "    rmse_med /=(len(janela_prevista))\n",
        "    mape_med /=(len(janela_prevista))\n",
        "    acuracia_med /=(len(janela_prevista))\n",
        "    \n",
        "    return round(mae_med,2), round(mse_med,2), round(rmse_med,2), round(mape_med,2), round(acuracia_med,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_validation)\n",
        "print(len(X_validation), len(y_pred), len(X_validation[0]))\n",
        "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_rescaled = scaler.inverse_transform(y_validation)\n",
        "#indices_to_validate = indices_to_validate[:len(y_pred_rescaled)]\n",
        "indices_to_validate = indices_to_validate[:validation_interval]\n",
        "print(indices_to_validate)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[0:output_steps], y_test_rescaled[0], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[0:output_steps], y_pred_rescaled[0], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,200000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[1:1+output_steps], y_test_rescaled[1], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[1:1+output_steps], y_pred_rescaled[1], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,200000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[2:2+output_steps], y_test_rescaled[2], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[2:2+output_steps], y_pred_rescaled[2], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,200000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[3:3+output_steps], y_test_rescaled[3], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[3:3+output_steps], y_pred_rescaled[3], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,200000,step=10000))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(indices_to_validate[4:4+output_steps], y_test_rescaled[4], label=\"Original\", color='blue')\n",
        "plt.plot(indices_to_validate[4:4+output_steps], y_pred_rescaled[4], label=\"Predição\", color='red')\n",
        "plt.yticks(np.arange(0,200000,step=10000))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avaliacao_multistep(y_test_rescaled,y_pred_rescaled)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rQy75p6HhZim",
        "qW4ExC6UnZz_",
        "sfY3t1p7oHSV",
        "E1yXXSWtDxuN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
